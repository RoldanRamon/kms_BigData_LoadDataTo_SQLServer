# ğŸš€ **Efficient Ways to Load Large Datasets into SQL Server** ğŸ—„ï¸

Handling large datasets can be challenging, especially when transferring them to a SQL Server hosted on-premises. This project demonstrates **three efficient ways** to upload datasets larger than **3 GB** from a local machine to an on-premises SQL Server. Whether you're dealing with performance bottlenecks or want to explore different approaches, these methods will help you move data quickly and effectively. ğŸ’¾

---

## âš¡ Aparklyr + dbplyr ğŸŒŸ

For large datasets, this approach leverages Apache Spark to distribute data processing, making it ideal for handling massive files (3 GB+). dbplyr allows you to write dplyr-like code that translates directly into SQL commands, all executed within the Spark environment. Here's a quick 

## ğŸï¸ Approach 3: data.table + ODBC + DBI ğŸ’¨

When speed is crucial, the combination of data.table and DBI delivers blazing performance. data.table is incredibly fast for in-memory operations, and DBI ensures a smooth and secure connection to your SQL Server.

## ğŸ” Why These Approaches?

Scalability: Handle datasets over 3 GB seamlessly.
Flexibility: Choose the approach that best suits your data structure and workflow.
Efficiency: Leverage R's ecosystem for both in-memory and distributed data processing.

## ğŸ“š Getting Started

1. Install the required packages:

```r
install.packages(c("sparklyr", "dplyr", "dbplyr", "DBI", "odbc", "data.table", "janitor"))
```

2. Clone this repository (SSH):

```r
git clone git@github.com:RoldanRamon/kms_BigData_LoadDataTo_SQLServer.git
```

3. Ensure you have Java installed for sparklyr:

Install Java for Windows
Install Java for Linux
Install Java for Mac

4. Run one of the provided scripts to see the methods in action!

## ğŸ—ï¸ Project Structure

```r
ğŸ“ root/
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â”œâ”€â”€ odbc_dplyr.R        # Approach 1: ODBC + dplyr
â”‚   â”œâ”€â”€ sparklyr_dbplyr.R   # Approach 2: Sparklyr + dbplyr
â”‚   â””â”€â”€ data_table_dbi.R    # Approach 3: data.table + DBI
â”‚   â”œâ”€â”€ initial_data_read.py  # Python script used to read the data initially
â”‚   â””â”€â”€ create_tables.sql   # SQL script to create tables with correct data types
â””â”€â”€ ğŸ“‚ data/
    â””â”€â”€ large-dataset.csv   # Example dataset (replace with your own)
```

### ğŸ Python Script (initial_data_read.py)
This Python script is used to read the large dataset before it's passed to R for further processing and loading into SQL Server.

### ğŸ›¢ï¸ SQL Script (create_tables.sql)
This SQL script is used to create the necessary tables in SQL Server with the correct primitive data types before loading the dataset.

## ğŸ“ Support
Feel free to raise an issue if you run into any problems or have questions. You can also reach me on LinkedIn or via email.

## ğŸ‰ Contributions
Contributions are welcome! If you'd like to improve the scripts or add new methods, feel free to open a pull request.